---
title: "As Books might Think"
author: "Federico Caria"
date: "2016-05-3"
tags: ["nlp", "ai"]
summary: "What A Book With 1 Neuron Could Do"
---

###### Introduction  
I just found a text in my archive that, in hindsight, I think I nailed. I'm republishing it as it was, even though some passages remain incomplete. This piece served as the premise for my talk at Cologne University, where I presented my doctoral thesis. At the time, I had already moved forward with my research and wanted to give the team a glimpse of the potential direction a postdoc might take. The result? A complete disaster ðŸ’¥ðŸ˜‚.

### 1. é»™æ®º
First-generation AI-powered tools will be getting out of the labs sometime quite soon. These enhancements come from recent breakthroughs in deep learning techniques, which promise to push the field of AI so far in the next ten years that every prediction about the future will have a kind of post-human flavor.
The USA, Russia, and China will soon run to arms.
While Putin [â€¦] the same guys who are most involved in AI research signed an open letter addressed to the United Nations to prevent "the third revolution in warfareâ€ by banning the development of all lethal autonomous weapon systems.
The letter is dated August 2017â€”We should not lose sight of the fact that, unlike other potential manifestations of AI which still remain in the realm of science fiction, autonomous weapons systems are on the cusp of development right nowâ€”Musk and company warn.
Flying drones, whose processors can react 100 times faster than a human. They can recognize your face and gently deposit 3 grams of explosive between your eyes in a few seconds.
They wonâ€™t say a word.

#### 1.1. An Attendant Gobot 
Despite governments and companies stepping forward to the next world war, I believe there is still a lot to do before such subscpecies of killers get smart enough to stitch a bullet in your a[*] while you're inyour home's toilet.
Building intelligent artificial agents mainly relies on figuring out what intelligence is, and nobody know that yet, apart from some, I believe, who think language is the key to the so-called AGI (Artificial General Intelligence). Now, it's really tempting to hesitate on this funny landscape but more important for us is to keep brainstorming the near future in order to figure out the ROI that we, as academics who have been involved in this field practically forever might enjoy. In fact, all this buzz, really plays to our advantage. Language research will soon be the bleeding edge! Can you believe it? One day you were in the basement of a dusty crappy department, the next day you find yourself among the highest paid researchers in industry and government. More and more, governments, big companies, and entrepreneurs will be putting computer scientists to work on natural language. More on this sci-fi in the next chapter.
Now forget Siri sillyness, forget IBMâ€™s Watson, Parsey McParseface, and all this prehistory. It's changing, here weâ€™re talking about something huge:

- **Keyword search will be replaced** together with the unnamable SEO by something called **"prompts"**.  Now Imagine a bash. Prompts are essentially bash commands written in natural language - a bit like texting your partner a grocery list - that are interpreted by the machine as if they were bash gibberish. People will interact with computers via those commands, the enthusiasts are already shouting that this is the end of the GUI but we'll have to wait and see how that evolves.

- We might move towards **a fuzzier idea of computer output**â€”evolution took the design risks that Von Neumann didnâ€™t! Embracing this change might need computers to take risks just liek biological systems do, in order to decentralized control, parallel processing, overlaid outputs, large amount of raw data. Contrary to computers, the brain is self-initiated (e.g., feedback loops) and has the ability to learn from experience.

- Natural language understanding and generation might go beyond statistical methods or require statistics capability **we are not equipped for**. If all this crap is gonna happen, it must happen somewhere outside of CPUs, and if I am right, a whole new market is going to open up for finding alternatives to CPU shortages.

#### 1.2. "Would Rather Like To Be A Philosophy Major"
I am quoting Mark Cuban here, one of these tech entrepreneurs, who believes critical thinking will become yet more valuable, as computers and robots increasingly replace human technical skills. The theme of 800 million global workers who will lose their jobs by 2030 and be replaced by robotic automation seems to be another trendy argument on the Web right in summer 2017. 
In a way, this is nothing new, since technology has been replacing human labor since the invention of the wheel, but this is going to happen little by little â€” it is already happening â€” starting from the less critical task - most likely the earliest outcome of recent experiments will likely hit the market with considerable caution, just tackling relatively low-level skill, low-wage, highly repetitive and ultra-boring work.
As for most people in high-skill positions, instead of being replaced wholesale, they will more likely find themselves working alongside their **synthetic colleagues**. 60% of todayâ€™s occupations have at least some portion that can be automated, and human assisted training of these systems produces more accurate results while training more robust systems going forward. 
This is the scenario we should expect for the next few years, before leveling up the game. We might try to forecast how that would be from the early expeiments: 

> Facebook is testing **project M** within its Messenger App to allow users to issue commands, access services, and make purchases through text input. 
> Mauro [2016]

A sort of command line, that figures out plain text. The most interesting thing about M is the concept of â€œhumans in the loop": this is key for us too, not only to imagine a good case of human-computer early symbiosis - here the baragin is all about human-generated tokens / computer performance - but to forecast a game that has a sort of two levels, where the next one is machines starting to want neurobiological data.
